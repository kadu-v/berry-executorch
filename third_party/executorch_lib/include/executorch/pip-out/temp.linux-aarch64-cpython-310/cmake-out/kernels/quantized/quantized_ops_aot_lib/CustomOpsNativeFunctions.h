/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */

// clang-format off
#pragma once

#include <tuple>

#include <ATen/ATen.h>
#include <torch/torch.h>

// @generated by torchgen/gen_executorch.py from NativeFunctions.h

namespace torch {
namespace executor {
namespace native {
TORCH_API at::Tensor & quantized_add_out(const at::Tensor & a, double a_scale, int64_t a_zero_point, int64_t a_quant_min, int64_t a_quant_max, const at::Tensor & b, double b_scale, int64_t b_zero_point, int64_t b_quant_min, int64_t b_quant_max, double out_scale, int64_t out_zero_point, int64_t out_quant_min, int64_t out_quant_max, at::Tensor & out);
TORCH_API ::std::tuple<at::Tensor &,at::Tensor &> choose_qparams_tensor_out(const at::Tensor & input, int64_t quant_min, int64_t quant_max, double eps, at::ScalarType dtype, at::Tensor & scale_out, at::Tensor & zero_point_out);
TORCH_API at::Tensor & dequantize_per_tensor_out(const at::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, at::ScalarType dtype, ::std::optional<at::ScalarType> out_dtype, at::Tensor & out);
TORCH_API at::Tensor & dequantize_per_tensor_tensor_args_out(const at::Tensor & input, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, at::ScalarType dtype, ::std::optional<at::ScalarType> out_dtype, at::Tensor & out);
TORCH_API at::Tensor & quantize_per_channel_out(const at::Tensor & input, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, at::ScalarType dtype, at::Tensor & out);
TORCH_API at::Tensor & dequantize_per_channel_out(const at::Tensor & input, const at::Tensor & scales, const ::std::optional<at::Tensor> & zero_points, int64_t axis, int64_t quant_min, int64_t quant_max, at::ScalarType dtype, ::std::optional<at::ScalarType> out_dtype, at::Tensor & out);
TORCH_API at::Tensor & quantized_mixed_mm_out(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & weight_scales, const ::std::optional<at::Tensor> & weight_zero_points, at::Tensor & out);
TORCH_API at::Tensor & quantized_mixed_linear_out(const at::Tensor & input, const at::Tensor & weight, const at::Tensor & weight_scales, const ::std::optional<at::Tensor> & weight_zero_points, ::std::optional<at::ScalarType> dtype, at::Tensor & out);
TORCH_API at::Tensor & quantize_per_tensor_out(const at::Tensor & input, double scale, int64_t zero_point, int64_t quant_min, int64_t quant_max, at::ScalarType dtype, at::Tensor & out);
TORCH_API at::Tensor & quantize_per_tensor_tensor_args_out(const at::Tensor & input, const at::Tensor & scale, const at::Tensor & zero_point, int64_t quant_min, int64_t quant_max, at::ScalarType dtype, at::Tensor & out);
} // namespace native
} // namespace executor
} // namespace torch
